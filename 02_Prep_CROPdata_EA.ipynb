{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FEWS NET Data Warehouse (FDW) Crop Data Preparation\n",
    "This script accesses and obtains crop production data from [FEWS NET Data Warehouse (FDW)](https://fdw.fews.net/en/).\n",
    "- Full API document is [here](https://fdw.fews.net/en/docs/users/api.html).\n",
    "- FEWS NET country administrative boundaries are [here](https://fews.net/fews-data/334).\n",
    "- [FAO - Food Security and Nutrition Analysis Unit - Somalia](https://fsnau.org/)\n",
    "\n",
    "By Donghoon Lee @ 12-03-2020"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import warnings\n",
    "from itertools import product\n",
    "from functools import reduce\n",
    "from configparser import ConfigParser\n",
    "from io import BytesIO, StringIO\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.PerformanceWarning)\n",
    "import geopandas as gpd\n",
    "import json\n",
    "import seaborn as sns\n",
    "from tools import save_hdf, cbarpam\n",
    "# Mapping\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "from mpl_toolkits.axes_grid1.inset_locator import inset_axes\n",
    "from matplotlib.colors import ListedColormap\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\", {'axes.linewidth': 1, 'grid.color': 'black'})\n",
    "\n",
    "# Function to invert a dictionary\n",
    "def invert_dict(d): \n",
    "    inverse = dict() \n",
    "    for key in d: \n",
    "        # Go through the list that is saved in the dict:\n",
    "        for item in d[key]:\n",
    "            # Check if in the inverted dict the key exists\n",
    "            if item not in inverse: \n",
    "                # If not create a new list\n",
    "                inverse[item] = key \n",
    "            else: \n",
    "                inverse[item].append(key) \n",
    "    return inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retreive crop production indicator value from FDW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Host address\n",
    "host = 'https://fdw.fews.net'\n",
    "auth = ('dlee', 'fdwdonghoon')\n",
    "# Crop list\n",
    "product_list = {'R01142AA': 'Sorghum',                  # KE, SO\n",
    "                'R01142AC': 'Maize Grain (White)',      # KE\n",
    "                'R01122AA': 'Maize (Corn)'}             # SO\n",
    "# Set up the parameters\n",
    "parameters = {'format': 'json',\n",
    "              'country': ['Somalia', 'Kenya'],\n",
    "              'survey_type': 'crop:best',\n",
    "              'product': ['R01122AA','R01122AC']}\n",
    "# Get the Crop Production Indicator Values (i.e. the list of Data Points)\n",
    "endpoint = '/api/cropproductionindicatorvalue/'\n",
    "response = requests.get(host + endpoint, auth=auth, params=parameters, proxies={})\n",
    "response.raise_for_status()\n",
    "# Turn the records into a Pandas DataFrame\n",
    "df = pd.DataFrame.from_records(response.json())\n",
    "retain_cols = ['country','fnid','admin_1','admin_2',\n",
    "               'population_group', 'crop_production_system',\n",
    "               'start_date','period_date',\n",
    "               'season_name','season_type','season_date','season_year',\n",
    "               'indicator','value','locality_name'] \n",
    "df = df[retain_cols]\n",
    "df = df.drop_duplicates().reset_index(drop=True)\n",
    "df_Kenya = df[df.country == 'Kenya']\n",
    "df_Somalia = df[df.country == 'Somalia']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Kenya\n",
    "Kenya crop seasonal calendar </br>\n",
    "![FEWS NET](https://fews.net/sites/default/files/styles/large/public/seasonal-calendar-kenya.png?itok=UpLuf1Bl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Relationships between administrative units\n",
    "- In Kenya, basd on the FEWS NET data, there were four times of changes in administrative units: 1982, 1989, 2009, and 2013.\n",
    "\n",
    "| Year | Admin-1 | # units  | Admin-2  | # units |\n",
    "| :---: | :----:  | :----:   | :----:   | :---:  |\n",
    "| 1982 | KE1982A1| 8        | KE1982A2 | 41      |\n",
    "| 1989 | KE1989A1| 8        | KE1989A2 | 47      |\n",
    "| 2009 | KE2009A1| 8        | KE2009A2 | 47      |\n",
    "| 2013 | **`KE2013A1`**| 47 | KE2013A2 | 270     |\n",
    "- In 1989, 6 districts are divided and added: Bomet, Makueni, Migori, Nyamira, Tharaka Nithi, and Vihiga.\n",
    "\n",
    "| 1982-1989 (original) |1989-2013 (changed)|1989-2013 (added)|\n",
    "| :---:| :---:|:---:|\n",
    "|Meru (KE1982A25020)    | Meru (KE1989A25017)    | Tharaka Nithi (KE1989A25018)|\n",
    "|Machakos (KE1982A25019)| Machakos(KE1989A25004) | Makueni       (KE1989A25005)|\n",
    "|Kericho  (KE1982A25424)| Kericho  (KE1989A25422)| Bomet         (KE1989A25420)|\n",
    "|Kissi    (KE1982A25319)| Kissi    (KE1989A25314)| Nyamira       (KE1989A25308)|\n",
    "|Homa Bay (KE1982A25318)| Homa Bay (KE1989A25313)| Migori        (KE1989A25316)|\n",
    "|Kakamega (KE1982A25512)| Kakamega (KE1989A25511)| Vihiga        (KE1989A25508)|\n",
    "\n",
    "- In 2013, Admin-2 became Admin-1 (No name changes).\n",
    "- Here, we use **`KE2013A1`** to represent all admin-level 1 crop data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FEWS NET shapfiles\n",
    "KE_adm1_2013 = gpd.read_file('./data/shapefile/KE_Admin1_2013.shp')\n",
    "KE_adm1_2013_name = KE_adm1_2013['ADMIN1'].values\n",
    "KE_adm2_1982 = gpd.read_file('./data/shapefile/KE_Admin2_1982.shp')\n",
    "KE_adm2_1982_name = KE_adm2_1982['ADMIN2'].values\n",
    "KE_adm2_1989 = gpd.read_file('./data/shapefile/KE_Admin2_1989.shp')\n",
    "KE_adm2_1989_name = KE_adm2_1989['ADMIN2'].values\n",
    "ADM1_LIST = list(KE_adm1_2013.FNID)\n",
    "\n",
    "# 1) Changes in 1989: New 6 districts are splitted/added \n",
    "#    : ['Bomet', 'Makueni', 'Migori', 'Nyamira', 'Tharaka Nithi', 'Vihiga']\n",
    "# KE_adm2_1989_name[~np.isin(KE_adm2_1989_name, KE_adm2_1982_name)]\n",
    "# Meru     (KE1982A25020) -> Meru     (KE1989A25017) and Tharaka Nithi (KE1989A25018)\n",
    "# Machakos (KE1982A25019) -> Machakos (KE1989A25004) and Makueni       (KE1989A25005)\n",
    "# Kericho  (KE1982A25424) -> Kericho  (KE1989A25422) and Bomet         (KE1989A25420)\n",
    "# Kissi    (KE1982A25319) -> Kissi    (KE1989A25314) and Nyamira       (KE1989A25308)\n",
    "# Homa Bay (KE1982A25318) -> Homa Bay (KE1989A25313) and Migori        (KE1989A25316)\n",
    "# Kakamega (KE1982A25512) -> Kakamega (KE1989A25511) and Vihiga        (KE1989A25508)\n",
    "adm_split1 = {'KE1982A25020':['KE1989A25017','KE1989A25018'],\n",
    "              'KE1982A25019':['KE1989A25004','KE1989A25005'],\n",
    "              'KE1982A25424':['KE1989A25422','KE1989A25420'],\n",
    "              'KE1982A25319':['KE1989A25314','KE1989A25308'],\n",
    "              'KE1982A25318':['KE1989A25313','KE1989A25316'],\n",
    "              'KE1982A25512':['KE1989A25511','KE1989A25508']}\n",
    "adm_split1 = invert_dict(adm_split1)\n",
    "\n",
    "### The second link is deprecated.\n",
    "# # 2) Only two adm-1 data exist before 2013: Nyanza and Nairobi\n",
    "# # Since FDW doesn't have adm-2 level for Nairobi, we can just link Nyanza.\n",
    "# adm_split2 = {'KE1982A153': ['KE1989A25313','KE1989A25314','KE1989A25315','KE1989A25316','KE1989A25308','KE1989A25317']}\n",
    "# adm_split2 = invert_dict(adm_split2)\n",
    "\n",
    "# 3) In 2013, adm-2 became adm-1 (No name changes)\n",
    "assert all(np.sort(KE_adm1_2013_name) == np.sort(KE_adm2_1989_name)) == True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Generate base tables of Area and Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate an initial data\n",
    "fnid_name = KE_adm1_2013[['FNID','ADMIN1']]\n",
    "df = df_Kenya.copy()\n",
    "\n",
    "# Loop by Name (we confirmed that names are the same)\n",
    "frame_area = []\n",
    "frame_prod = []\n",
    "for i, (fnid_org, name_org) in fnid_name.iterrows():\n",
    "    # Admin level 1\n",
    "    subdf = df[df.admin_1 == name_org]\n",
    "    time = pd.to_datetime(subdf['season_date'].values)\n",
    "    # Check if data has more than 3 records in a year\n",
    "    assert (np.unique(time,return_counts=True)[1] > 3).sum() == 0\n",
    "    pivot_adm1 = pd.pivot_table(subdf, values='value', index='season_date', columns=['indicator'], aggfunc=sum)\n",
    "    # Admin level 2\n",
    "    subdf = df[df.admin_2 == name_org]\n",
    "    time = pd.to_datetime(subdf['season_date'].values)\n",
    "    # Check if data has more than 4 season records in a year\n",
    "    assert (np.unique(time,return_counts=True)[1] > 3).sum() == 0\n",
    "    pivot_adm2 = pd.pivot_table(subdf, values='value', index='season_date', columns=['indicator'], aggfunc=sum)\n",
    "    # Base pivot table\n",
    "    pivot = pd.concat((pivot_adm2, pivot_adm1), axis=0)\n",
    "    # Remove season if either area and production is missing\n",
    "    pivot = pivot[pivot.isna().sum(1) == 0]\n",
    "\n",
    "    # Fill missing annual data with available short/long data\n",
    "    # - if both short/long seasons exist, we do summation\n",
    "    # - if a single season exists, we just copy it as annual data\n",
    "    time = pd.to_datetime(pivot.index)\n",
    "    pivot['year'], pivot['month'] = time.year, time.month\n",
    "    pivot_sub = pivot[pivot.month != 10].groupby('year').sum().reset_index(drop=False)\n",
    "    pivot_sub['month'], pivot_sub['day'] = 10, 1\n",
    "    pivot_sub.index = pd.to_datetime(pivot_sub[['year', 'month','day']]).dt.strftime('%Y-%m-%d')\n",
    "    pivot_sub.index.name = 'season_date'\n",
    "    pivot_sub = pivot_sub[['Area Planted','Quantity Produced','Yield','year','month']]\n",
    "    pivot = pd.concat((pivot, pivot_sub),axis=0).sort_index()\n",
    "\n",
    "    # Final area and production tables\n",
    "    table_area = pd.pivot_table(pivot, index='year',columns='month',values='Area Planted')\n",
    "    table_area = pd.DataFrame(data=table_area, index=table_area.index, columns=[2,7,10])\n",
    "    table_prod = pd.pivot_table(pivot, index='year',columns='month',values='Quantity Produced')\n",
    "    table_prod = pd.DataFrame(data=table_prod, index=table_prod.index, columns=[2,7,10])\n",
    "\n",
    "    # MultiIndex\n",
    "    tuples = [(fnid_org, name_org, 'Short', '03-31'),\n",
    "              (fnid_org, name_org, 'Long', '08-31'),\n",
    "              (fnid_org, name_org, 'Annual', '01-31')]\n",
    "    mdx = pd.MultiIndex.from_tuples(tuples, names=['FNID','name','season','harvest'])\n",
    "    table_area.columns = mdx\n",
    "    table_prod.columns = mdx\n",
    "    frame_area.append(table_area)\n",
    "    frame_prod.append(table_prod)\n",
    "area = pd.concat(frame_area, axis=1)\n",
    "prod = pd.concat(frame_prod, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Import external datasets\n",
    "- \"National Maize Production 2014 - 2019.xlsx\"\n",
    "- \"KE_FEWS_Production data up to_Apr2015.xlsx\"\n",
    "\n",
    "These datasets are prepared by Liya Manga (FEWS NET) and slightly modified (mostly removing unrealistic ticks) by Donghoon Lee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (a) Import data table of \"National Maize Production 2014 - 2019.xlsx\"\n",
    "fn = './data/crop/forwarded/National Maize Production 2014 - 2019_modified_dlee.xlsx'\n",
    "df = pd.read_excel(fn, sheet_name='modified_by_dlee', header=[0,1,2,3], index_col=0)\n",
    "# Import 2017-2019 (FDW has data until 2016)\n",
    "df = df.loc(axis=1)[pd.IndexSlice[np.arange(2017,2020),:,:,:]]\n",
    "# Convert the number of 90kg bags to tons\n",
    "df.loc(axis=1)[pd.IndexSlice[:,:,'Production',:]] = df.loc(axis=1)[pd.IndexSlice[:,:,'Production',:]] * 90/1000\n",
    "df.columns = df.columns.droplevel(3)\n",
    "# Insert area and production\n",
    "for em in product([2017,2018,2019],['Long','Short'],['Area','Production']):\n",
    "    year, season, indicator = em[0], em[1], em[2]\n",
    "    sdata = df[(year,season,indicator)]\n",
    "    for name, value in sdata.iteritems():\n",
    "        if indicator == 'Area':\n",
    "            area.loc[year,pd.IndexSlice[:,name,season,:]] = value\n",
    "        elif indicator == 'Production':\n",
    "            prod.loc[year,pd.IndexSlice[:,name,season,:]] = value\n",
    "# Aggregate annual values\n",
    "for year in [2017,2018,2019]:\n",
    "    area_annual = np.nansum([area.loc[year,pd.IndexSlice[:,:,['Short'],:]].values,\n",
    "                             area.loc[year,pd.IndexSlice[:,:,['Long'],:]].values],axis=0)\n",
    "    area.loc[year,pd.IndexSlice[:,:,['Annual'],:]] = area_annual\n",
    "    prod_annual = np.nansum([prod.loc[year,pd.IndexSlice[:,:,['Short'],:]].values,\n",
    "                             prod.loc[year,pd.IndexSlice[:,:,['Long'],:]].values],axis=0)\n",
    "    prod.loc[year,pd.IndexSlice[:,:,['Annual'],:]] = prod_annual\n",
    "\n",
    "    \n",
    "# (b) Import data table of \"KE_FEWS_Production data up to_Apr2015.xlsx\"\n",
    "fn = './data/crop/forwarded/KE_FEWS_Production data up to_Apr2015_modified_dlee.xlsx'\n",
    "df = pd.read_excel(fn, sheet_name='modified_by_dlee', header=[0,1], index_col=0)\n",
    "df[df == 0] = np.nan\n",
    "# Remove years with all missing records\n",
    "df = df[~(df.isna().sum(1) == df.shape[1])]\n",
    "names = df.columns.get_level_values(0).unique()\n",
    "# Extend time period of base tables\n",
    "area = area.reindex(np.arange(df.index.min(), area.index.max()+1))\n",
    "prod = prod.reindex(np.arange(df.index.min(), prod.index.max()+1))\n",
    "for name in names:\n",
    "    # Find missing years of base tables\n",
    "    temp = area.loc[:,pd.IndexSlice[:,name,'Annual',:]]\n",
    "    year_missing = temp.index[temp.isna().values.flatten()]\n",
    "    year_missing = year_missing[year_missing <= df.index.max()]\n",
    "    # If the Excel file has a record (both area and production) for the missing year, import it\n",
    "    for year in year_missing:\n",
    "        sub = df.loc[year, pd.IndexSlice[name,:]]\n",
    "        if sub.notna().sum() == 2:\n",
    "            area.loc[year,pd.IndexSlice[:,name,'Annual',:]] = sub.loc[pd.IndexSlice[name, 'Area']]\n",
    "            prod.loc[year,pd.IndexSlice[:,name,'Annual',:]] = sub.loc[pd.IndexSlice[name, 'Production']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Link yield between original and divided districts\n",
    "Here, we just copy the yield record (not area and production) of the original districts to the divided districts which are currently missing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate yield (yield = production / area)\n",
    "crop = prod/area\n",
    "df_raw = df_Kenya.copy()\n",
    "# Copy historical records of the original district to the divided district\n",
    "for new, old in adm_split1.items():\n",
    "    df_raw[df_raw.fnid == old]\n",
    "    if len(df_raw[df_raw.fnid == old]) > 0:\n",
    "        year_end = int(df_raw[df_raw.fnid == old].season_date.max()[:4])\n",
    "        name_new = df_raw[df_raw.fnid == new].admin_2.unique()[0]\n",
    "        name_old = df_raw[df_raw.fnid == old].admin_2.unique()[0]\n",
    "        crop.loc[:year_end, pd.IndexSlice[:, name_new ,'Annual',:]] = crop.loc[:year_end, pd.IndexSlice[:, name_old ,'Annual',:]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Finalize yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/crop/KE_maize_adm0_yield_annual.hdf is saved.\n",
      "./data/crop/KE_maize_adm1_yield_annual.hdf is saved.\n",
      "./data/crop/KE_maize_adm1_area_annual.hdf is saved.\n",
      "./data/crop/KE_maize_adm1_prod_annual.hdf is saved.\n",
      "./data/crop/KE_maize_summary.xlsx is saved..\n"
     ]
    }
   ],
   "source": [
    "# Re-define names\n",
    "area1, prod1, crop1 = area, prod, crop\n",
    "\n",
    "# Calculate national annual yield (annual total production / annual total planted area)\n",
    "ntnl_annual = pd.DataFrame(index = area1.index, columns = ['Area','Production','Yield','Miss_districts'])\n",
    "for year in ntnl_annual.index:\n",
    "    ntnl_annual.loc[year, 'Area'] = area1.loc[year,pd.IndexSlice[:,:,'Annual',:]].sum()\n",
    "    ntnl_annual.loc[year, 'Production'] = prod1.loc[year,pd.IndexSlice[:,:,'Annual',:]].sum()\n",
    "    nmiss = area1.loc[year,pd.IndexSlice[:,:,'Annual',:]].isna().sum()\n",
    "    ntnl_annual.loc[year, 'Miss_districts'] = nmiss / len(ADM1_LIST) *100\n",
    "ntnl_annual['Yield'] = ntnl_annual['Production'] / ntnl_annual['Area']\n",
    "\n",
    "# Annual and seasonal (districts and national)\n",
    "yield1a = crop1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; yield1a.columns = yield1a.columns.droplevel([1,2,3])\n",
    "area1a = area1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; area1a.columns = area1a.columns.droplevel([1,2,3])\n",
    "prod1a = prod1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; prod1a.columns = prod1a.columns.droplevel([1,2,3])\n",
    "\n",
    "# Export to HDF files\n",
    "# National\n",
    "save_hdf('./data/crop/KE_maize_adm0_yield_annual.hdf', ntnl_annual)\n",
    "# Admin-1 level\n",
    "save_hdf('./data/crop/KE_maize_adm1_yield_annual.hdf', yield1a)\n",
    "save_hdf('./data/crop/KE_maize_adm1_area_annual.hdf', area1a)\n",
    "save_hdf('./data/crop/KE_maize_adm1_prod_annual.hdf', prod1a)\n",
    "\n",
    "# Export to Excel file\n",
    "fn_out = './data/crop/KE_maize_summary.xlsx'\n",
    "with pd.ExcelWriter(fn_out, engine='xlsxwriter') as writer:\n",
    "    # National\n",
    "    ntnl_annual.to_excel(writer, sheet_name='adm0_yield_annual')\n",
    "    # Admin-1 level\n",
    "    yield1a.to_excel(writer, sheet_name='adm1_yield_annual')\n",
    "    area1a.to_excel(writer, sheet_name='adm1_area_annual')\n",
    "    prod1a.to_excel(writer, sheet_name='adm1_prod_annual')\n",
    "\n",
    "    print('%s is saved..' % fn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Somalia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somalia crop seasonal calendar </br>\n",
    "![FEWS NET](https://fews.net/sites/default/files/styles/large/public/seasonal-calendar-somalia.png?itok=6LQ1arWc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Relationships between administrative units\n",
    "- In Somalia, based on FEWS NET data, only 1990 administrative units are available.\n",
    "\n",
    "| Year | Admin-1 | # units | Admin-2  | # units |\n",
    "| :--- | :----:  | :----:  | :----:   | ---:    |\n",
    "| 1990 | **`SO1990A1`**| 18      | SO1990A2 | 74      |\n",
    "- Most of reliable data are admin-2 level, and FDW has few data for admin-1 level. Therefore, we aggregate/merge admin-2 level data to represent admin-1 level.\n",
    "- Afmadow (SO1990A22802) has two crop reporting units: Afmadow (SO1990R22805) and Afmadow (Hagar) (SO1990R22806). This is confirmed by FEWS NET crop reporting unit (./data/admin_boundaries/SO_Crop_Units_1990.shp). Since SO1990A22802 has only one off-season record, we aggregate SO1990R22805 and SO1990R22806 to replace SO1990A22802.\n",
    "\n",
    "| District |Crop reporting units|\n",
    "| :---:|:---:|\n",
    "|Afmadow (SO1990A22802) | Afmadow (SO1990R22805) and  Hagar (SO1990R22806)|\n",
    "- Here, we use **`SO1990A1`** to represent all admin-level 1 crop data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FEWS NET shapfiles\n",
    "SO_adm1_1990 = gpd.read_file('./data/shapefile/SO_Admin1_1990.shp')   # Total 18 FNID\n",
    "SO_adm2_1990 = gpd.read_file('./data/shapefile/SO_Admin2_1990.shp')   # Total 74 FNID\n",
    "ADM1_LIST = SO_adm1_1990.FNID.to_list()\n",
    "ADM2_LIST = SO_adm2_1990.FNID.to_list()\n",
    "# Join FNIDs between Admin-1 and Admin-2\n",
    "join = SO_adm2_1990.merge(SO_adm1_1990[['FNID','ADMIN1']], on='ADMIN1', suffixes=['_adm2', '_adm1'])\n",
    "join = join[['FNID_adm1','FNID_adm2']]\n",
    "adm_joint = join.groupby('FNID_adm1')['FNID_adm2'].apply(lambda g: g.values.tolist()).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Generate base tables of Area and Production\n",
    "- Frank: Issue with post-2010 data reporting separately for Riverine / Agro-pastoral Areas (Kimetrica) â€“ will this affect upload. \n",
    "- Annwer: No, it will not affect upload. The livelihood zone is a locality. The FNID, pop_group, and livelihood zone collumns collectivly define the location\n",
    "- Here, we aggregate both Riverine and Agro-pastoral areas per season to represent seasonal value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CreateMultiIndex(fnid, name):\n",
    "    # MultiIndex\n",
    "    tuples = [(fnid, name, 'Deyr', '02-28'),\n",
    "              (fnid, name, 'Deyr-off', '03-31'),\n",
    "              (fnid, name, 'Gu', '08-31'),\n",
    "              (fnid, name, 'Gu-off', '10-31'),\n",
    "              (fnid, name, 'Annual', '12-31')]\n",
    "    mdx = pd.MultiIndex.from_tuples(tuples, names=['FNID','name','season','harvest'])\n",
    "    return mdx\n",
    "\n",
    "# Generate an initial data\n",
    "df = df_Somalia.copy()\n",
    "# Name \"normal\" crop production system\n",
    "df.loc[df['crop_production_system'].isna(), 'crop_production_system'] = 'normal'\n",
    "# Remove \"Area Planted\" rows\n",
    "df = df.loc[~df['indicator'].isin(['Area Planted', 'Yield'])]\n",
    "\n",
    "# List of FNID and Name \n",
    "fnid1_data = df.loc[df.admin_2.isna(), ['fnid','admin_1']].drop_duplicates().reset_index(drop=True)\n",
    "fnid1_data.columns = ['fnid', 'name']\n",
    "fnid2_data = df.loc[df.admin_2.notna(), ['fnid','admin_2']].drop_duplicates().reset_index(drop=True)\n",
    "fnid2_data.columns = ['fnid', 'name']\n",
    "fnid_data = pd.concat([fnid1_data, fnid2_data],axis=0)\n",
    "\n",
    "# Loop by FNID\n",
    "frame_area = []\n",
    "frame_prod = []\n",
    "for i, (fnid, name) in fnid_data.iterrows():\n",
    "    subdf = df[(df.fnid == fnid)].reset_index()\n",
    "    # Remove data if either \"Area Harvested\" or \"Quantity Produced\" is missing in each \n",
    "    # \"crop_production_system\" and \"season_date\" combination\n",
    "    comb = subdf[['crop_production_system','season_date']].drop_duplicates()\n",
    "    for i, (system, season) in comb.iterrows():\n",
    "        targ = (subdf['crop_production_system'] == system) & (subdf['season_date'] == season)\n",
    "        temp = subdf.loc[targ, 'value'].values\n",
    "        if len(temp) != 2 or np.isnan(temp).sum() > 0:\n",
    "            subdf = subdf[~targ].reset_index(drop=True)\n",
    "    if len(subdf) == 0: continue        \n",
    "    # Pivot table\n",
    "    time = pd.to_datetime(subdf['season_date'].values)\n",
    "    pivot = pd.pivot_table(subdf, values='value', index='season_date', columns=['indicator'], aggfunc=sum)\n",
    "    time = pd.to_datetime(pivot.index)\n",
    "    pivot['year'], pivot['month'] = time.year, time.month\n",
    "    # Convert to time-series table\n",
    "    table_area = pd.pivot_table(pivot, index='year',columns='month',values='Area Harvested')\n",
    "    table_area = pd.DataFrame(data=table_area, index=table_area.index, columns=[1,3,7,9,12])\n",
    "    table_area[12] = table_area[table_area.columns[:-1]].sum(1)\n",
    "    table_prod = pd.pivot_table(pivot, index='year',columns='month',values='Quantity Produced')\n",
    "    table_prod = pd.DataFrame(data=table_prod, index=table_prod.index, columns=[1,3,7,9,12])\n",
    "    table_prod[12] = table_prod[table_prod.columns[:-1]].sum(1)\n",
    "    # MultiIndex\n",
    "    mdx = CreateMultiIndex(fnid, name)\n",
    "    table_area.columns = mdx\n",
    "    table_prod.columns = mdx\n",
    "    frame_area.append(table_area)\n",
    "    frame_prod.append(table_prod)\n",
    "area = pd.concat(frame_area, axis=1)\n",
    "prod = pd.concat(frame_prod, axis=1)\n",
    "# Reindex year\n",
    "area = area.reindex(np.arange(area.index.min(), 2020))\n",
    "prod = prod.reindex(np.arange(prod.index.min(), 2020))\n",
    "\n",
    "# Correct crop reporting units \n",
    "# Summation Afmadow (SO1990R22805) and Hagar (SO1990R22806) to represent Afmadow (SO1990A22802)\n",
    "area_temp = area['SO1990R22805'].add(area['SO1990R22806'], fill_value=0)\n",
    "prod_temp = prod['SO1990R22805'].add(prod['SO1990R22806'], fill_value=0)\n",
    "area['SO1990A22802'] = area['SO1990A22802'].add(area_temp, fill_value=0)\n",
    "prod['SO1990A22802'] = prod['SO1990A22802'].add(prod_temp, fill_value=0)\n",
    "area = area.drop(['SO1990R22805', 'SO1990R22806'], axis=1)\n",
    "prod = prod.drop(['SO1990R22805', 'SO1990R22806'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) Manual removing unrealistic records\n",
    "In Admin-2 level data, we found unrealistic records. So, here we manually remove those records including:\n",
    "- 26 districts have the exactly same `1 mt/ha` yield values in `Deyr` season of `2001`.\n",
    "- Three districts have unrealistic yield values in `Gu` season of `2005`: Bu'aale (11.65 mt/ha), Jilib (2.27 mt/ha), and Saakow (20.8 mt/ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unrealistic values in 2001\n",
    "crop = prod/area\n",
    "temp = crop.loc[2001,pd.IndexSlice[:,:,'Deyr',:]]\n",
    "remove_index = temp.index[temp == 1]\n",
    "prod.loc[2001,remove_index] = np.nan\n",
    "area.loc[2001,remove_index] = np.nan\n",
    "# Remove unrealistic values in 2005\n",
    "area.loc[2005,pd.IndexSlice[:,[\"Bu'aale\",\"Jilib\",\"Saakow\"],'Gu',:]] = np.nan\n",
    "prod.loc[2005,pd.IndexSlice[:,[\"Bu'aale\",\"Jilib\",\"Saakow\"],'Gu',:]] = np.nan\n",
    "\n",
    "# Recalculate annual total area and production\n",
    "for fnid in area.columns.get_level_values(0).unique():\n",
    "    temp = area.loc[:, pd.IndexSlice[fnid,:,['Deyr','Deyr-off','Gu','Gu-off'],:]]\n",
    "    resum = temp.sum(1); resum[temp.isna().sum(1) == 4] = np.nan\n",
    "    area.loc[:, pd.IndexSlice[fnid,:,'Annual',:]] = resum.values\n",
    "    temp = prod.loc[:, pd.IndexSlice[fnid,:,['Deyr','Deyr-off','Gu','Gu-off'],:]]\n",
    "    resum = temp.sum(1); resum[temp.isna().sum(1) == 4] = np.nan\n",
    "    prod.loc[:, pd.IndexSlice[fnid,:,'Annual',:]] = resum.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Aggregate Admin-2 level data to Admin-1 level\n",
    "In the above section, we made the area and production tables for all available Admin 1 and 2 levels. In this section, we merge Admin-2 level data to reconstruct Admin-1 level with including the existing Admin-1 level data. Then we will split Admin-2 level data from the entire tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnid_data = area.columns.get_level_values(0).unique()\n",
    "area_frame = []\n",
    "prod_frame = []\n",
    "# Loop per FNID_ADM1\n",
    "for i, (fnid, name) in SO_adm1_1990[['FNID', 'ADMIN1']].iterrows():\n",
    "    # - Merge avaliable data\n",
    "    aval = fnid_data[fnid_data.isin([fnid, *adm_joint[fnid]])]\n",
    "    if len(aval) == 0: continue \n",
    "    aval_area_frame = []\n",
    "    aval_prod_frame = []\n",
    "    for i in aval:\n",
    "        temp_area = area[i]\n",
    "        temp_prod = prod[i]\n",
    "        temp_area.columns = temp_area.columns.droplevel([0])\n",
    "        temp_prod.columns = temp_prod.columns.droplevel([0])\n",
    "        aval_area_frame.append(temp_area)\n",
    "        aval_prod_frame.append(temp_prod)\n",
    "    merged_area = reduce(lambda  x, y: x.add(y, fill_value=0), aval_area_frame)\n",
    "    merged_prod = reduce(lambda  x, y: x.add(y, fill_value=0), aval_prod_frame)\n",
    "    merged_area.columns = CreateMultiIndex(fnid, name)\n",
    "    merged_prod.columns = CreateMultiIndex(fnid, name)\n",
    "    area_frame.append(merged_area)\n",
    "    prod_frame.append(merged_prod)\n",
    "area1 = pd.concat(area_frame, axis=1)\n",
    "prod1 = pd.concat(prod_frame, axis=1)\n",
    "# Finally select Admin-2 level from the entire pool\n",
    "fnid_data_adm2 = fnid_data[~fnid_data.isin(adm_joint.keys())]\n",
    "area2 = area[fnid_data_adm2]\n",
    "prod2 = prod[fnid_data_adm2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Finalize yield data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/crop/SO_maize_adm0_yield_annual.hdf is saved.\n",
      "./data/crop/SO_maize_adm0_yield_deyr.hdf is saved.\n",
      "./data/crop/SO_maize_adm0_yield_gu.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_yield_annual.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_yield_deyr.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_yield_gu.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_area_annual.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_area_deyr.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_area_gu.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_prod_annual.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_prod_deyr.hdf is saved.\n",
      "./data/crop/SO_maize_adm1_prod_gu.hdf is saved.\n",
      "./data/crop/SO_maize_summary.xlsx is saved..\n"
     ]
    }
   ],
   "source": [
    "# Calculate yield\n",
    "crop1 = prod1 / area1\n",
    "crop2 = prod2 / area2\n",
    "\n",
    "# Calculate national yield (total production / total planted area) in both seasons and annual\n",
    "nmiss_default = len(ADM1_LIST) - len(area1.columns.get_level_values(0).unique())\n",
    "ntnl_annual = pd.DataFrame(index = area1.index, columns = ['Area','Production','Yield','Miss_districts'])\n",
    "ntnl_deyr = ntnl_annual.copy()\n",
    "ntnl_gu = ntnl_annual.copy()\n",
    "for year in ntnl_annual.index:\n",
    "    # Annual\n",
    "    ntnl_annual.loc[year, 'Area'] = area1.loc[year,pd.IndexSlice[:,:,'Annual',:]].sum()\n",
    "    ntnl_annual.loc[year, 'Production'] = prod1.loc[year,pd.IndexSlice[:,:,'Annual',:]].sum()\n",
    "    nmiss = area1.loc[year,pd.IndexSlice[:,:,'Annual',:]].isna().sum()\n",
    "    ntnl_annual.loc[year, 'Miss_districts'] = (nmiss + nmiss_default)/len(ADM1_LIST)*100\n",
    "    # Deyr season\n",
    "    ntnl_deyr.loc[year, 'Area'] = area1.loc[year,pd.IndexSlice[:,:,'Deyr',:]].sum()\n",
    "    ntnl_deyr.loc[year, 'Production'] = prod1.loc[year,pd.IndexSlice[:,:,'Deyr',:]].sum()\n",
    "    nmiss = area1.loc[year,pd.IndexSlice[:,:,'Deyr',:]].isna().sum()\n",
    "    ntnl_deyr.loc[year, 'Miss_districts'] = (nmiss + nmiss_default)/len(ADM1_LIST)*100\n",
    "    # Gu season\n",
    "    ntnl_gu.loc[year, 'Area'] = area1.loc[year,pd.IndexSlice[:,:,'Gu',:]].sum()\n",
    "    ntnl_gu.loc[year, 'Production'] = prod1.loc[year,pd.IndexSlice[:,:,'Gu',:]].sum()\n",
    "    nmiss = area1.loc[year,pd.IndexSlice[:,:,'Gu',:]].isna().sum()\n",
    "    ntnl_gu.loc[year, 'Miss_districts'] = (nmiss + nmiss_default)/len(ADM1_LIST)*100\n",
    "ntnl_annual['Yield'] = ntnl_annual['Production'] / ntnl_annual['Area']\n",
    "ntnl_deyr['Yield'] = ntnl_deyr['Production'] / ntnl_deyr['Area']\n",
    "ntnl_gu['Yield'] = ntnl_gu['Production'] / ntnl_gu['Area']\n",
    "ntnl_annual[ntnl_annual == 0] = np.nan\n",
    "ntnl_deyr[ntnl_deyr == 0] = np.nan\n",
    "ntnl_gu[ntnl_gu == 0] = np.nan\n",
    "\n",
    "# Annual and seasonal (districts and national)\n",
    "yield1a = crop1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; yield1a.columns = yield1a.columns.droplevel([1,2,3])\n",
    "area1a = area1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; area1a.columns = area1a.columns.droplevel([1,2,3])\n",
    "prod1a = prod1.loc[:,pd.IndexSlice[:,:,'Annual',:]]; prod1a.columns = prod1a.columns.droplevel([1,2,3])\n",
    "yield1s = crop1.loc[:,pd.IndexSlice[:,:,'Deyr',:]]; yield1s.columns = yield1s.columns.droplevel([1,2,3])\n",
    "area1s = area1.loc[:,pd.IndexSlice[:,:,'Deyr',:]]; area1s.columns = area1s.columns.droplevel([1,2,3])\n",
    "prod1s = prod1.loc[:,pd.IndexSlice[:,:,'Deyr',:]]; prod1s.columns = prod1s.columns.droplevel([1,2,3])\n",
    "yield1l = crop1.loc[:,pd.IndexSlice[:,:,'Gu',:]]; yield1l.columns = yield1l.columns.droplevel([1,2,3])\n",
    "area1l = area1.loc[:,pd.IndexSlice[:,:,'Gu',:]]; area1l.columns = area1l.columns.droplevel([1,2,3])\n",
    "prod1l = prod1.loc[:,pd.IndexSlice[:,:,'Gu',:]]; prod1l.columns = prod1l.columns.droplevel([1,2,3])\n",
    "\n",
    "# Export to HDF files\n",
    "# National\n",
    "save_hdf('./data/crop/SO_maize_adm0_yield_annual.hdf', ntnl_annual)\n",
    "save_hdf('./data/crop/SO_maize_adm0_yield_deyr.hdf', ntnl_deyr)\n",
    "save_hdf('./data/crop/SO_maize_adm0_yield_gu.hdf', ntnl_gu)\n",
    "# Admin-1 level\n",
    "save_hdf('./data/crop/SO_maize_adm1_yield_annual.hdf', yield1a)\n",
    "save_hdf('./data/crop/SO_maize_adm1_yield_deyr.hdf', yield1s)\n",
    "save_hdf('./data/crop/SO_maize_adm1_yield_gu.hdf', yield1l)\n",
    "save_hdf('./data/crop/SO_maize_adm1_area_annual.hdf', area1a)\n",
    "save_hdf('./data/crop/SO_maize_adm1_area_deyr.hdf', area1s)\n",
    "save_hdf('./data/crop/SO_maize_adm1_area_gu.hdf', area1l)\n",
    "save_hdf('./data/crop/SO_maize_adm1_prod_annual.hdf', prod1a)\n",
    "save_hdf('./data/crop/SO_maize_adm1_prod_deyr.hdf', prod1s)\n",
    "save_hdf('./data/crop/SO_maize_adm1_prod_gu.hdf', prod1l)\n",
    "\n",
    "# Export to Excel file\n",
    "fn_out = './data/crop/SO_maize_summary.xlsx'\n",
    "with pd.ExcelWriter(fn_out, engine='xlsxwriter') as writer:\n",
    "    # National\n",
    "    ntnl_annual.to_excel(writer, sheet_name='adm0_yield_annual')\n",
    "    ntnl_deyr.to_excel(writer, sheet_name='adm0_yield_deyr')\n",
    "    ntnl_gu.to_excel(writer, sheet_name='adm0_yield_gu')\n",
    "    # Admin-1 level\n",
    "    yield1a.to_excel(writer, sheet_name='adm1_yield_annual')\n",
    "    area1a.to_excel(writer, sheet_name='adm1_area_annual')\n",
    "    prod1a.to_excel(writer, sheet_name='adm1_prod_annual')\n",
    "    yield1s.to_excel(writer, sheet_name='adm1_yield_deyr')\n",
    "    area1s.to_excel(writer, sheet_name='adm1_area_deyr')\n",
    "    prod1s.to_excel(writer, sheet_name='adm1_prod_deyr')\n",
    "    yield1l.to_excel(writer, sheet_name='adm1_yield_gu')\n",
    "    area1l.to_excel(writer, sheet_name='adm1_area_gu')\n",
    "    prod1l.to_excel(writer, sheet_name='adm1_prod_gu')\n",
    "    print('%s is saved..' % fn_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Complete dataset of Maize yield for East Africa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Merge shapefiles of Kenya and Somalia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./data/shapefile/EA_admin1.shp is saved.\n"
     ]
    }
   ],
   "source": [
    "# Merge shapefiles\n",
    "KE_adm1_2013 = gpd.read_file('./data/shapefile/KE_Admin1_2013.shp')\n",
    "SO_adm1_1990 = gpd.read_file('./data/shapefile/SO_Admin1_1990.shp')\n",
    "EA_adm1 = pd.concat([KE_adm1_2013, SO_adm1_1990], axis=0)[['FNID', 'ADMIN0', 'ADMIN1', 'geometry']]\n",
    "fn_out = './data/shapefile/EA_admin1.shp'\n",
    "EA_adm1.to_file(fn_out)\n",
    "print('%s is saved.' % fn_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
